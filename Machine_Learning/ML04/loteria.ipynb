{"cells":[{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\julo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Bidirectional, Dropout\n","\n","pd.options.mode.chained_assignment = None"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n"]}],"source":["tf.config.list_physical_devices('GPU')\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137837,"status":"ok","timestamp":1703171930779,"user":{"displayName":"Julian Dworzycki","userId":"08277126832162080478"},"user_tz":-60},"id":"hmmzvjYvPNLt","outputId":"e39355e7-c7d7-42f0-c8b1-0a9c85dc7088"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6971 entries, 0 to 6970\n","Data columns (total 8 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   GAME    6971 non-null   int64 \n"," 1   DATE    6971 non-null   object\n"," 2   A       6971 non-null   int64 \n"," 3   B       6971 non-null   int64 \n"," 4   C       6971 non-null   int64 \n"," 5   D       6971 non-null   int64 \n"," 6   E       6971 non-null   int64 \n"," 7   F       6971 non-null   int64 \n","dtypes: int64(7), object(1)\n","memory usage: 435.8+ KB\n","Epoch 1/200\n","70/70 - 12s - loss: 1.0009 - accuracy: 0.1732 - 12s/epoch - 178ms/step\n","Epoch 2/200\n","70/70 - 5s - loss: 1.0002 - accuracy: 0.1862 - 5s/epoch - 65ms/step\n","Epoch 3/200\n","70/70 - 5s - loss: 0.9997 - accuracy: 0.1827 - 5s/epoch - 65ms/step\n","Epoch 4/200\n","70/70 - 5s - loss: 0.9996 - accuracy: 0.1861 - 5s/epoch - 65ms/step\n","Epoch 5/200\n","70/70 - 5s - loss: 0.9997 - accuracy: 0.1903 - 5s/epoch - 65ms/step\n","Epoch 6/200\n","70/70 - 5s - loss: 0.9994 - accuracy: 0.2040 - 5s/epoch - 65ms/step\n","Epoch 7/200\n","70/70 - 5s - loss: 0.9995 - accuracy: 0.1851 - 5s/epoch - 65ms/step\n","Epoch 8/200\n","70/70 - 5s - loss: 0.9991 - accuracy: 0.2007 - 5s/epoch - 65ms/step\n","Epoch 9/200\n","70/70 - 5s - loss: 0.9993 - accuracy: 0.1874 - 5s/epoch - 65ms/step\n","Epoch 10/200\n","70/70 - 5s - loss: 0.9990 - accuracy: 0.1960 - 5s/epoch - 65ms/step\n","Epoch 11/200\n","70/70 - 5s - loss: 0.9987 - accuracy: 0.1990 - 5s/epoch - 65ms/step\n","Epoch 12/200\n","70/70 - 5s - loss: 0.9986 - accuracy: 0.1964 - 5s/epoch - 65ms/step\n","Epoch 13/200\n","70/70 - 5s - loss: 0.9988 - accuracy: 0.1950 - 5s/epoch - 65ms/step\n","Epoch 14/200\n","70/70 - 5s - loss: 0.9985 - accuracy: 0.1997 - 5s/epoch - 65ms/step\n","Epoch 15/200\n","70/70 - 5s - loss: 0.9984 - accuracy: 0.2002 - 5s/epoch - 65ms/step\n","Epoch 16/200\n","70/70 - 5s - loss: 0.9981 - accuracy: 0.1972 - 5s/epoch - 65ms/step\n","Epoch 17/200\n","70/70 - 5s - loss: 0.9982 - accuracy: 0.1993 - 5s/epoch - 65ms/step\n","Epoch 18/200\n","70/70 - 5s - loss: 0.9983 - accuracy: 0.2059 - 5s/epoch - 65ms/step\n","Epoch 19/200\n","70/70 - 5s - loss: 0.9979 - accuracy: 0.1842 - 5s/epoch - 65ms/step\n","Epoch 20/200\n","70/70 - 5s - loss: 0.9980 - accuracy: 0.2099 - 5s/epoch - 65ms/step\n","Epoch 21/200\n","70/70 - 5s - loss: 0.9978 - accuracy: 0.1953 - 5s/epoch - 65ms/step\n","Epoch 22/200\n","70/70 - 5s - loss: 0.9972 - accuracy: 0.1976 - 5s/epoch - 65ms/step\n","Epoch 23/200\n","70/70 - 5s - loss: 0.9973 - accuracy: 0.1977 - 5s/epoch - 65ms/step\n","Epoch 24/200\n","70/70 - 5s - loss: 0.9971 - accuracy: 0.2035 - 5s/epoch - 65ms/step\n","Epoch 25/200\n","70/70 - 5s - loss: 0.9968 - accuracy: 0.2036 - 5s/epoch - 65ms/step\n","Epoch 26/200\n","70/70 - 5s - loss: 0.9967 - accuracy: 0.1977 - 5s/epoch - 65ms/step\n","Epoch 27/200\n","70/70 - 5s - loss: 0.9964 - accuracy: 0.2006 - 5s/epoch - 65ms/step\n","Epoch 28/200\n","70/70 - 5s - loss: 0.9962 - accuracy: 0.2052 - 5s/epoch - 66ms/step\n","Epoch 29/200\n","70/70 - 5s - loss: 0.9955 - accuracy: 0.2000 - 5s/epoch - 65ms/step\n","Epoch 30/200\n","70/70 - 5s - loss: 0.9956 - accuracy: 0.2002 - 5s/epoch - 65ms/step\n","Epoch 31/200\n","70/70 - 5s - loss: 0.9960 - accuracy: 0.1939 - 5s/epoch - 65ms/step\n","Epoch 32/200\n","70/70 - 5s - loss: 0.9949 - accuracy: 0.1999 - 5s/epoch - 65ms/step\n","Epoch 33/200\n","70/70 - 5s - loss: 0.9945 - accuracy: 0.2012 - 5s/epoch - 65ms/step\n","Epoch 34/200\n","70/70 - 5s - loss: 0.9949 - accuracy: 0.2005 - 5s/epoch - 66ms/step\n","Epoch 35/200\n","70/70 - 5s - loss: 0.9940 - accuracy: 0.1953 - 5s/epoch - 65ms/step\n","Epoch 36/200\n","70/70 - 5s - loss: 0.9940 - accuracy: 0.2075 - 5s/epoch - 65ms/step\n","Epoch 37/200\n","70/70 - 5s - loss: 0.9935 - accuracy: 0.2003 - 5s/epoch - 66ms/step\n","Epoch 38/200\n","70/70 - 5s - loss: 0.9933 - accuracy: 0.2052 - 5s/epoch - 65ms/step\n","Epoch 39/200\n","70/70 - 5s - loss: 0.9928 - accuracy: 0.1957 - 5s/epoch - 65ms/step\n","Epoch 40/200\n","70/70 - 5s - loss: 0.9929 - accuracy: 0.2023 - 5s/epoch - 65ms/step\n","Epoch 41/200\n","70/70 - 5s - loss: 0.9914 - accuracy: 0.2020 - 5s/epoch - 66ms/step\n","Epoch 42/200\n","70/70 - 5s - loss: 0.9916 - accuracy: 0.1977 - 5s/epoch - 65ms/step\n","Epoch 43/200\n","70/70 - 5s - loss: 0.9907 - accuracy: 0.2019 - 5s/epoch - 65ms/step\n","Epoch 44/200\n","70/70 - 5s - loss: 0.9918 - accuracy: 0.2005 - 5s/epoch - 65ms/step\n","Epoch 45/200\n","70/70 - 5s - loss: 0.9912 - accuracy: 0.2074 - 5s/epoch - 66ms/step\n","Epoch 46/200\n","70/70 - 5s - loss: 0.9899 - accuracy: 0.2025 - 5s/epoch - 65ms/step\n","Epoch 47/200\n","70/70 - 5s - loss: 0.9909 - accuracy: 0.2040 - 5s/epoch - 66ms/step\n","Epoch 48/200\n","70/70 - 5s - loss: 0.9899 - accuracy: 0.2128 - 5s/epoch - 65ms/step\n","Epoch 49/200\n","70/70 - 5s - loss: 0.9894 - accuracy: 0.2132 - 5s/epoch - 66ms/step\n","Epoch 50/200\n","70/70 - 5s - loss: 0.9886 - accuracy: 0.2098 - 5s/epoch - 66ms/step\n","Epoch 51/200\n","70/70 - 5s - loss: 0.9873 - accuracy: 0.2030 - 5s/epoch - 65ms/step\n","Epoch 52/200\n","70/70 - 5s - loss: 0.9883 - accuracy: 0.2039 - 5s/epoch - 66ms/step\n","Epoch 53/200\n","70/70 - 5s - loss: 0.9874 - accuracy: 0.2045 - 5s/epoch - 66ms/step\n","Epoch 54/200\n","70/70 - 5s - loss: 0.9863 - accuracy: 0.2025 - 5s/epoch - 66ms/step\n","Epoch 55/200\n","70/70 - 5s - loss: 0.9878 - accuracy: 0.2038 - 5s/epoch - 65ms/step\n","Epoch 56/200\n","70/70 - 5s - loss: 0.9856 - accuracy: 0.2141 - 5s/epoch - 65ms/step\n","Epoch 57/200\n","70/70 - 5s - loss: 0.9852 - accuracy: 0.2046 - 5s/epoch - 65ms/step\n","Epoch 58/200\n","70/70 - 5s - loss: 0.9855 - accuracy: 0.2015 - 5s/epoch - 65ms/step\n","Epoch 59/200\n","70/70 - 5s - loss: 0.9845 - accuracy: 0.2072 - 5s/epoch - 65ms/step\n","Epoch 60/200\n","70/70 - 5s - loss: 0.9826 - accuracy: 0.2085 - 5s/epoch - 65ms/step\n","Epoch 61/200\n","70/70 - 5s - loss: 0.9825 - accuracy: 0.2046 - 5s/epoch - 65ms/step\n","Epoch 62/200\n","70/70 - 6s - loss: 0.9804 - accuracy: 0.2049 - 6s/epoch - 86ms/step\n","Epoch 63/200\n","70/70 - 5s - loss: 0.9811 - accuracy: 0.2112 - 5s/epoch - 68ms/step\n","Epoch 64/200\n","70/70 - 5s - loss: 0.9800 - accuracy: 0.2072 - 5s/epoch - 64ms/step\n","Epoch 65/200\n","70/70 - 5s - loss: 0.9792 - accuracy: 0.2140 - 5s/epoch - 64ms/step\n","Epoch 66/200\n","70/70 - 4s - loss: 0.9796 - accuracy: 0.2033 - 4s/epoch - 64ms/step\n","Epoch 67/200\n","70/70 - 5s - loss: 0.9768 - accuracy: 0.2089 - 5s/epoch - 65ms/step\n","Epoch 68/200\n","70/70 - 5s - loss: 0.9767 - accuracy: 0.2148 - 5s/epoch - 65ms/step\n","Epoch 69/200\n","70/70 - 4s - loss: 0.9748 - accuracy: 0.2170 - 4s/epoch - 64ms/step\n","Epoch 70/200\n","70/70 - 5s - loss: 0.9738 - accuracy: 0.2128 - 5s/epoch - 64ms/step\n","Epoch 71/200\n","70/70 - 5s - loss: 0.9731 - accuracy: 0.2124 - 5s/epoch - 64ms/step\n","Epoch 72/200\n","70/70 - 4s - loss: 0.9711 - accuracy: 0.2178 - 4s/epoch - 64ms/step\n","Epoch 73/200\n","70/70 - 5s - loss: 0.9706 - accuracy: 0.2151 - 5s/epoch - 64ms/step\n","Epoch 74/200\n","70/70 - 4s - loss: 0.9690 - accuracy: 0.2148 - 4s/epoch - 64ms/step\n","Epoch 75/200\n","70/70 - 5s - loss: 0.9684 - accuracy: 0.2140 - 5s/epoch - 64ms/step\n","Epoch 76/200\n","70/70 - 5s - loss: 0.9668 - accuracy: 0.2141 - 5s/epoch - 64ms/step\n","Epoch 77/200\n","70/70 - 5s - loss: 0.9655 - accuracy: 0.2250 - 5s/epoch - 64ms/step\n","Epoch 78/200\n","70/70 - 5s - loss: 0.9612 - accuracy: 0.2154 - 5s/epoch - 64ms/step\n","Epoch 79/200\n","70/70 - 5s - loss: 0.9616 - accuracy: 0.2171 - 5s/epoch - 64ms/step\n","Epoch 80/200\n","70/70 - 4s - loss: 0.9608 - accuracy: 0.2193 - 4s/epoch - 64ms/step\n","Epoch 81/200\n","70/70 - 5s - loss: 0.9580 - accuracy: 0.2157 - 5s/epoch - 64ms/step\n","Epoch 82/200\n","70/70 - 5s - loss: 0.9545 - accuracy: 0.2138 - 5s/epoch - 65ms/step\n","Epoch 83/200\n","70/70 - 5s - loss: 0.9556 - accuracy: 0.2206 - 5s/epoch - 64ms/step\n","Epoch 84/200\n","70/70 - 5s - loss: 0.9525 - accuracy: 0.2206 - 5s/epoch - 64ms/step\n","Epoch 85/200\n","70/70 - 4s - loss: 0.9521 - accuracy: 0.2220 - 4s/epoch - 64ms/step\n","Epoch 86/200\n","70/70 - 4s - loss: 0.9454 - accuracy: 0.2242 - 4s/epoch - 64ms/step\n","Epoch 87/200\n","70/70 - 5s - loss: 0.9480 - accuracy: 0.2265 - 5s/epoch - 65ms/step\n","Epoch 88/200\n","70/70 - 5s - loss: 0.9452 - accuracy: 0.2242 - 5s/epoch - 64ms/step\n","Epoch 89/200\n","70/70 - 5s - loss: 0.9405 - accuracy: 0.2260 - 5s/epoch - 64ms/step\n","Epoch 90/200\n","70/70 - 5s - loss: 0.9373 - accuracy: 0.2239 - 5s/epoch - 64ms/step\n","Epoch 91/200\n","70/70 - 4s - loss: 0.9339 - accuracy: 0.2323 - 4s/epoch - 64ms/step\n","Epoch 92/200\n","70/70 - 4s - loss: 0.9323 - accuracy: 0.2296 - 4s/epoch - 64ms/step\n","Epoch 93/200\n","70/70 - 4s - loss: 0.9272 - accuracy: 0.2249 - 4s/epoch - 64ms/step\n","Epoch 94/200\n","70/70 - 5s - loss: 0.9266 - accuracy: 0.2352 - 5s/epoch - 64ms/step\n","Epoch 95/200\n","70/70 - 4s - loss: 0.9226 - accuracy: 0.2355 - 4s/epoch - 64ms/step\n","Epoch 96/200\n","70/70 - 5s - loss: 0.9201 - accuracy: 0.2336 - 5s/epoch - 64ms/step\n","Epoch 97/200\n","70/70 - 5s - loss: 0.9166 - accuracy: 0.2328 - 5s/epoch - 64ms/step\n","Epoch 98/200\n","70/70 - 4s - loss: 0.9147 - accuracy: 0.2414 - 4s/epoch - 64ms/step\n","Epoch 99/200\n","70/70 - 5s - loss: 0.9101 - accuracy: 0.2326 - 5s/epoch - 64ms/step\n","Epoch 100/200\n","70/70 - 5s - loss: 0.9046 - accuracy: 0.2427 - 5s/epoch - 67ms/step\n","Epoch 101/200\n","70/70 - 5s - loss: 0.9024 - accuracy: 0.2407 - 5s/epoch - 64ms/step\n","Epoch 102/200\n","70/70 - 5s - loss: 0.8980 - accuracy: 0.2401 - 5s/epoch - 64ms/step\n","Epoch 103/200\n","70/70 - 5s - loss: 0.8934 - accuracy: 0.2431 - 5s/epoch - 64ms/step\n","Epoch 104/200\n","70/70 - 5s - loss: 0.8896 - accuracy: 0.2526 - 5s/epoch - 65ms/step\n","Epoch 105/200\n","70/70 - 5s - loss: 0.8870 - accuracy: 0.2534 - 5s/epoch - 64ms/step\n","Epoch 106/200\n","70/70 - 5s - loss: 0.8812 - accuracy: 0.2530 - 5s/epoch - 64ms/step\n","Epoch 107/200\n","70/70 - 5s - loss: 0.8736 - accuracy: 0.2504 - 5s/epoch - 64ms/step\n","Epoch 108/200\n","70/70 - 5s - loss: 0.8686 - accuracy: 0.2533 - 5s/epoch - 65ms/step\n","Epoch 109/200\n","70/70 - 5s - loss: 0.8617 - accuracy: 0.2545 - 5s/epoch - 65ms/step\n","Epoch 110/200\n","70/70 - 5s - loss: 0.8549 - accuracy: 0.2563 - 5s/epoch - 65ms/step\n","Epoch 111/200\n","70/70 - 4s - loss: 0.8493 - accuracy: 0.2559 - 4s/epoch - 64ms/step\n","Epoch 112/200\n","70/70 - 5s - loss: 0.8436 - accuracy: 0.2678 - 5s/epoch - 64ms/step\n","Epoch 113/200\n","70/70 - 5s - loss: 0.8358 - accuracy: 0.2605 - 5s/epoch - 65ms/step\n","Epoch 114/200\n","70/70 - 5s - loss: 0.8300 - accuracy: 0.2713 - 5s/epoch - 65ms/step\n","Epoch 115/200\n","70/70 - 5s - loss: 0.8195 - accuracy: 0.2790 - 5s/epoch - 65ms/step\n","Epoch 116/200\n","70/70 - 5s - loss: 0.8166 - accuracy: 0.2783 - 5s/epoch - 65ms/step\n","Epoch 117/200\n","70/70 - 5s - loss: 0.8056 - accuracy: 0.2814 - 5s/epoch - 65ms/step\n","Epoch 118/200\n","70/70 - 5s - loss: 0.7985 - accuracy: 0.2846 - 5s/epoch - 65ms/step\n","Epoch 119/200\n","70/70 - 5s - loss: 0.7905 - accuracy: 0.2882 - 5s/epoch - 65ms/step\n","Epoch 120/200\n","70/70 - 5s - loss: 0.7784 - accuracy: 0.2955 - 5s/epoch - 64ms/step\n","Epoch 121/200\n","70/70 - 5s - loss: 0.7721 - accuracy: 0.2895 - 5s/epoch - 64ms/step\n","Epoch 122/200\n","70/70 - 5s - loss: 0.7646 - accuracy: 0.2957 - 5s/epoch - 64ms/step\n","Epoch 123/200\n","70/70 - 5s - loss: 0.7524 - accuracy: 0.3089 - 5s/epoch - 65ms/step\n","Epoch 124/200\n","70/70 - 5s - loss: 0.7412 - accuracy: 0.3053 - 5s/epoch - 65ms/step\n","Epoch 125/200\n","70/70 - 5s - loss: 0.7320 - accuracy: 0.3067 - 5s/epoch - 65ms/step\n","Epoch 126/200\n","70/70 - 5s - loss: 0.7220 - accuracy: 0.3143 - 5s/epoch - 65ms/step\n","Epoch 127/200\n","70/70 - 5s - loss: 0.7102 - accuracy: 0.3158 - 5s/epoch - 65ms/step\n","Epoch 128/200\n","70/70 - 5s - loss: 0.7038 - accuracy: 0.3195 - 5s/epoch - 64ms/step\n","Epoch 129/200\n","70/70 - 5s - loss: 0.6913 - accuracy: 0.3232 - 5s/epoch - 64ms/step\n","Epoch 130/200\n","70/70 - 5s - loss: 0.6839 - accuracy: 0.3165 - 5s/epoch - 65ms/step\n","Epoch 131/200\n","70/70 - 5s - loss: 0.6708 - accuracy: 0.3346 - 5s/epoch - 65ms/step\n","Epoch 132/200\n","70/70 - 5s - loss: 0.6577 - accuracy: 0.3379 - 5s/epoch - 65ms/step\n","Epoch 133/200\n","70/70 - 5s - loss: 0.6427 - accuracy: 0.3389 - 5s/epoch - 65ms/step\n","Epoch 134/200\n","70/70 - 5s - loss: 0.6335 - accuracy: 0.3485 - 5s/epoch - 65ms/step\n","Epoch 135/200\n","70/70 - 5s - loss: 0.6222 - accuracy: 0.3478 - 5s/epoch - 65ms/step\n","Epoch 136/200\n","70/70 - 5s - loss: 0.6065 - accuracy: 0.3563 - 5s/epoch - 64ms/step\n","Epoch 137/200\n","70/70 - 5s - loss: 0.5955 - accuracy: 0.3606 - 5s/epoch - 64ms/step\n","Epoch 138/200\n","70/70 - 5s - loss: 0.5887 - accuracy: 0.3660 - 5s/epoch - 65ms/step\n","Epoch 139/200\n","70/70 - 5s - loss: 0.5742 - accuracy: 0.3637 - 5s/epoch - 65ms/step\n","Epoch 140/200\n","70/70 - 5s - loss: 0.5628 - accuracy: 0.3716 - 5s/epoch - 64ms/step\n","Epoch 141/200\n","70/70 - 5s - loss: 0.5450 - accuracy: 0.3765 - 5s/epoch - 65ms/step\n","Epoch 142/200\n","70/70 - 5s - loss: 0.5362 - accuracy: 0.3767 - 5s/epoch - 64ms/step\n","Epoch 143/200\n","70/70 - 5s - loss: 0.5196 - accuracy: 0.3880 - 5s/epoch - 65ms/step\n","Epoch 144/200\n","70/70 - 5s - loss: 0.5063 - accuracy: 0.3940 - 5s/epoch - 64ms/step\n","Epoch 145/200\n","70/70 - 4s - loss: 0.4984 - accuracy: 0.3998 - 4s/epoch - 64ms/step\n","Epoch 146/200\n","70/70 - 5s - loss: 0.4848 - accuracy: 0.4012 - 5s/epoch - 65ms/step\n","Epoch 147/200\n","70/70 - 5s - loss: 0.4770 - accuracy: 0.4041 - 5s/epoch - 64ms/step\n","Epoch 148/200\n","70/70 - 5s - loss: 0.4608 - accuracy: 0.4088 - 5s/epoch - 64ms/step\n","Epoch 149/200\n","70/70 - 5s - loss: 0.4485 - accuracy: 0.4184 - 5s/epoch - 64ms/step\n","Epoch 150/200\n","70/70 - 5s - loss: 0.4333 - accuracy: 0.4166 - 5s/epoch - 65ms/step\n","Epoch 151/200\n","70/70 - 5s - loss: 0.4244 - accuracy: 0.4207 - 5s/epoch - 64ms/step\n","Epoch 152/200\n","70/70 - 5s - loss: 0.4105 - accuracy: 0.4273 - 5s/epoch - 65ms/step\n","Epoch 153/200\n","70/70 - 5s - loss: 0.4011 - accuracy: 0.4339 - 5s/epoch - 65ms/step\n","Epoch 154/200\n","70/70 - 5s - loss: 0.3898 - accuracy: 0.4423 - 5s/epoch - 65ms/step\n","Epoch 155/200\n","70/70 - 5s - loss: 0.3812 - accuracy: 0.4459 - 5s/epoch - 65ms/step\n","Epoch 156/200\n","70/70 - 5s - loss: 0.3680 - accuracy: 0.4466 - 5s/epoch - 65ms/step\n","Epoch 157/200\n","70/70 - 5s - loss: 0.3546 - accuracy: 0.4535 - 5s/epoch - 64ms/step\n","Epoch 158/200\n","70/70 - 5s - loss: 0.3458 - accuracy: 0.4568 - 5s/epoch - 65ms/step\n","Epoch 159/200\n","70/70 - 5s - loss: 0.3357 - accuracy: 0.4617 - 5s/epoch - 65ms/step\n","Epoch 160/200\n","70/70 - 5s - loss: 0.3280 - accuracy: 0.4594 - 5s/epoch - 65ms/step\n","Epoch 161/200\n","70/70 - 5s - loss: 0.3183 - accuracy: 0.4753 - 5s/epoch - 65ms/step\n","Epoch 162/200\n","70/70 - 5s - loss: 0.3071 - accuracy: 0.4813 - 5s/epoch - 65ms/step\n","Epoch 163/200\n","70/70 - 5s - loss: 0.3046 - accuracy: 0.4865 - 5s/epoch - 65ms/step\n","Epoch 164/200\n","70/70 - 5s - loss: 0.2863 - accuracy: 0.4944 - 5s/epoch - 64ms/step\n","Epoch 165/200\n","70/70 - 5s - loss: 0.2824 - accuracy: 0.5078 - 5s/epoch - 65ms/step\n","Epoch 166/200\n","70/70 - 5s - loss: 0.2751 - accuracy: 0.4980 - 5s/epoch - 65ms/step\n","Epoch 167/200\n","70/70 - 5s - loss: 0.2662 - accuracy: 0.5037 - 5s/epoch - 65ms/step\n","Epoch 168/200\n","70/70 - 5s - loss: 0.2641 - accuracy: 0.5065 - 5s/epoch - 64ms/step\n","Epoch 169/200\n","70/70 - 5s - loss: 0.2537 - accuracy: 0.5079 - 5s/epoch - 68ms/step\n","Epoch 170/200\n","70/70 - 5s - loss: 0.2469 - accuracy: 0.5218 - 5s/epoch - 67ms/step\n","Epoch 171/200\n","70/70 - 5s - loss: 0.2383 - accuracy: 0.5346 - 5s/epoch - 67ms/step\n","Epoch 172/200\n","70/70 - 5s - loss: 0.2346 - accuracy: 0.5207 - 5s/epoch - 65ms/step\n","Epoch 173/200\n","70/70 - 5s - loss: 0.2261 - accuracy: 0.5366 - 5s/epoch - 65ms/step\n","Epoch 174/200\n","70/70 - 5s - loss: 0.2224 - accuracy: 0.5370 - 5s/epoch - 65ms/step\n","Epoch 175/200\n","70/70 - 5s - loss: 0.2169 - accuracy: 0.5370 - 5s/epoch - 65ms/step\n","Epoch 176/200\n","70/70 - 5s - loss: 0.2109 - accuracy: 0.5534 - 5s/epoch - 65ms/step\n","Epoch 177/200\n","70/70 - 5s - loss: 0.2050 - accuracy: 0.5503 - 5s/epoch - 64ms/step\n","Epoch 178/200\n","70/70 - 5s - loss: 0.2032 - accuracy: 0.5553 - 5s/epoch - 65ms/step\n","Epoch 179/200\n","70/70 - 5s - loss: 0.1962 - accuracy: 0.5518 - 5s/epoch - 65ms/step\n","Epoch 180/200\n","70/70 - 5s - loss: 0.1941 - accuracy: 0.5711 - 5s/epoch - 64ms/step\n","Epoch 181/200\n","70/70 - 4s - loss: 0.1867 - accuracy: 0.5681 - 4s/epoch - 64ms/step\n","Epoch 182/200\n","70/70 - 5s - loss: 0.1857 - accuracy: 0.5752 - 5s/epoch - 64ms/step\n","Epoch 183/200\n","70/70 - 5s - loss: 0.1773 - accuracy: 0.5806 - 5s/epoch - 64ms/step\n","Epoch 184/200\n","70/70 - 5s - loss: 0.1754 - accuracy: 0.5847 - 5s/epoch - 65ms/step\n","Epoch 185/200\n","70/70 - 5s - loss: 0.1729 - accuracy: 0.5916 - 5s/epoch - 65ms/step\n","Epoch 186/200\n","70/70 - 4s - loss: 0.1689 - accuracy: 0.5968 - 4s/epoch - 64ms/step\n","Epoch 187/200\n","70/70 - 5s - loss: 0.1674 - accuracy: 0.6080 - 5s/epoch - 65ms/step\n","Epoch 188/200\n","70/70 - 5s - loss: 0.1610 - accuracy: 0.6031 - 5s/epoch - 65ms/step\n","Epoch 189/200\n","70/70 - 5s - loss: 0.1589 - accuracy: 0.6055 - 5s/epoch - 65ms/step\n","Epoch 190/200\n","70/70 - 5s - loss: 0.1551 - accuracy: 0.6077 - 5s/epoch - 65ms/step\n","Epoch 191/200\n","70/70 - 5s - loss: 0.1502 - accuracy: 0.6136 - 5s/epoch - 65ms/step\n","Epoch 192/200\n","70/70 - 5s - loss: 0.1489 - accuracy: 0.6190 - 5s/epoch - 64ms/step\n","Epoch 193/200\n","70/70 - 5s - loss: 0.1469 - accuracy: 0.6190 - 5s/epoch - 65ms/step\n","Epoch 194/200\n","70/70 - 5s - loss: 0.1438 - accuracy: 0.6255 - 5s/epoch - 64ms/step\n","Epoch 195/200\n","70/70 - 5s - loss: 0.1440 - accuracy: 0.6267 - 5s/epoch - 65ms/step\n","Epoch 196/200\n","70/70 - 5s - loss: 0.1375 - accuracy: 0.6430 - 5s/epoch - 64ms/step\n","Epoch 197/200\n","70/70 - 5s - loss: 0.1342 - accuracy: 0.6312 - 5s/epoch - 65ms/step\n","Epoch 198/200\n","70/70 - 5s - loss: 0.1345 - accuracy: 0.6396 - 5s/epoch - 65ms/step\n","Epoch 199/200\n","70/70 - 5s - loss: 0.1304 - accuracy: 0.6541 - 5s/epoch - 64ms/step\n","Epoch 200/200\n","70/70 - 5s - loss: 0.1292 - accuracy: 0.6498 - 5s/epoch - 64ms/step\n","1/1 [==============================] - 1s 1s/step\n","\n","Przewidziane numery:\t [ 6 13 17 25 29 45]\n","Prawdziwe numery:\t [ 4 14 15 23 28 44]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Bidirectional, Dropout\n","\n","pd.options.mode.chained_assignment = None\n","\n","df = pd.read_csv(\"./totolotek.csv\")\n","# df = pd.read_csv(\"/content/drive/MyDrive/totolotek.csv\")\n","\n","df.head()\n","df.tail()\n","df.info()\n","df.describe()\n","df.drop(['GAME', 'DATE'], axis=1, inplace=True)\n","df.head()\n","\n","scaler = StandardScaler().fit(df.values)\n","transformed_dataset = scaler.transform(df.values)\n","transformed_df = pd.DataFrame(data=transformed_dataset, index=df.index)\n","transformed_df.head()\n","number_of_rows = df.values.shape[0]\n","window_length = 7\n","number_of_features = df.values.shape[1]\n","\n","X = np.empty([ number_of_rows - window_length, window_length, number_of_features], dtype=float)\n","y = np.empty([ number_of_rows - window_length, number_of_features], dtype=float)\n","\n","for i in range(0, number_of_rows - window_length):\n","    X[i] = transformed_df.iloc[i : i+window_length, 0 : number_of_features]\n","    y[i] = transformed_df.iloc[i+window_length : i+window_length+1, 0 : number_of_features]\n","\n","model = Sequential()\n","\n","model.add(Bidirectional(LSTM(240,\n","                        input_shape = (window_length, number_of_features),\n","                        return_sequences = True)))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(240,\n","                        input_shape = (window_length, number_of_features),\n","                        return_sequences = True)))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(240,\n","                        input_shape = (window_length, number_of_features),\n","                        return_sequences = True)))\n","model.add(Bidirectional(LSTM(240,\n","                        input_shape = (window_length, number_of_features),\n","                        return_sequences = False)))\n","\n","model.add(Dropout(0.2))\n","model.add(Dense(59))\n","model.add(Dense(number_of_features))\n","\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss ='mse', metrics=['accuracy'])\n","model.fit(x=X, y=y, batch_size=100, epochs=200, verbose=2)\n","\n","to_predict = df.tail(8)\n","to_predict.drop([to_predict.index[-1]],axis=0, inplace=True)\n","to_predict = np.array(to_predict)\n","scaled_to_predict = scaler.transform(to_predict)\n","\n","y_pred = model.predict(np.array([scaled_to_predict]))\n","print(\"\\nPrzewidziane numery:\\t\", scaler.inverse_transform(y_pred).astype(int)[0])\n","prediction = df.tail(1)\n","prediction = np.array(prediction)\n","print(\"Prawdziwe numery:\\t\", prediction[0])"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
